{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import scatter\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate 50 random points from multivariate gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x15ef1f3ac18>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xx = np.array([0, 1])\n",
    "yy = np.array([0, 1])\n",
    "means = [xx.mean(), yy.mean()]  \n",
    "stds = [xx.std()/3, yy.std()/3]\n",
    "corr = 0.7         # correlation\n",
    "covs = [[stds[0]**2          , stds[0]*stds[1]*corr], \n",
    "        [stds[0]*stds[1]*corr,           stds[1]**2]] \n",
    "\n",
    "m = np.random.multivariate_normal(means, covs, 50).T\n",
    "scatter(m[0], m[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(np.array([m.T[:,0]]).T, m.T[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a linear model and here is the precise model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.85409588]), 0.054489543186794664)"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_, model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8540958829540322, 0.054489543186794664)"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_theta_hat = (model.coef_[0], model.intercept_)\n",
    "exact_theta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(m.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(k, b):\n",
    "    return sum((df[0]*k+b - df[1])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k is the slope and b is the intercept\n",
    "# Compute the gradient on the total loss (dL/dk, dL/db)\n",
    "\n",
    "def loss_gradient(k, b, df):\n",
    "    return (sum((df[0]*k+b - df[1])*2*df[0])/10, sum((df[0]*k+b - df[1])*2)/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter1\n",
      "(15.208668516596726, 29.925188822063756)\n",
      "(3, 2)\n",
      "iter2\n",
      "(-5.30868059564893, -11.155029193985733)\n",
      "(1.3270464631743601, -1.2917707704270134)\n",
      "iter3\n",
      "(2.2337936216716874, 3.9646780497947907)\n",
      "(1.9110013286957424, -0.06471755908858268)\n",
      "iter4\n",
      "(-0.5486030450981582, -1.5953474735964008)\n",
      "(1.6652840303118568, -0.5008321445660097)\n",
      "iter5\n",
      "(0.4683742441350288, 0.4539706480304053)\n",
      "(1.7256303652726541, -0.32534392247040556)\n",
      "iter6\n",
      "(0.08748399580978186, -0.29677407347095935)\n",
      "(1.674109198417801, -0.37528069375375017)\n",
      "iter7\n",
      "(0.22112830965580826, -0.017275356278052788)\n",
      "(1.6644859588787249, -0.34263554567194465)\n",
      "iter8\n",
      "(0.1656046916096919, -0.11695228869883915)\n",
      "(1.640161844816586, -0.34073525648135883)\n",
      "iter9\n",
      "(0.1798474077312291, -0.07718500423323331)\n",
      "(1.6219453287395198, -0.32787050472448653)\n",
      "iter10\n",
      "(0.16857355511496888, -0.08880581520305422)\n",
      "(1.6021621138890847, -0.3193801542588309)\n",
      "iter11\n",
      "(0.16684008527220634, -0.0815930441240212)\n",
      "(1.5836190228264382, -0.3096115145864949)\n",
      "iter12\n",
      "(0.16174345641607193, -0.08138396592493305)\n",
      "(1.5652666134464954, -0.30063627973285256)\n",
      "iter13\n",
      "(0.1580280362571334, -0.0786695070328826)\n",
      "(1.5474748332407275, -0.29168404348110993)\n",
      "iter14\n",
      "(0.1539444603492169, -0.07694688254526087)\n",
      "(1.5300917492524428, -0.28303039770749283)\n",
      "iter15\n",
      "(0.15013292489082178, -0.07492748499295583)\n",
      "(1.513157858614029, -0.2745662406275141)\n",
      "iter16\n",
      "(0.14635443776095344, -0.07308376917689785)\n",
      "(1.4966432368760387, -0.266324217278289)\n",
      "iter17\n",
      "(0.14269360368766398, -0.07124022226407262)\n",
      "(1.480544248722334, -0.2582850026688302)\n",
      "iter18\n",
      "(0.13911603846069678, -0.06945980299501013)\n",
      "(1.4648479523166908, -0.2504485782197822)\n",
      "iter19\n",
      "(0.135631223208704, -0.06771776144949929)\n",
      "(1.4495451880860142, -0.2428079998903311)\n",
      "iter20\n",
      "(0.1322325775756119, -0.06602166115248148)\n",
      "(1.4346257535330567, -0.23535904613088618)\n",
      "iter21\n",
      "(0.1289195087431033, -0.06436721406660204)\n",
      "(1.4200801699997394, -0.22809666340411322)\n",
      "iter22\n",
      "(0.12568929620659894, -0.06275453082931048)\n",
      "(1.405899024037998, -0.221016269856787)\n",
      "iter23\n",
      "(0.12254007600677279, -0.06118214027884624)\n",
      "(1.3920732014552721, -0.21411327146556283)\n",
      "iter24\n",
      "(0.11946974078997825, -0.0596491891422211)\n",
      "(1.3785937930945271, -0.20738323603488976)\n",
      "iter25\n",
      "(0.11647634274962207, -0.05815463172754755)\n",
      "(1.3654521216076294, -0.20082182522924544)\n",
      "iter26\n",
      "(0.11355794360401616, -0.05669752721392)\n",
      "(1.352639723905171, -0.19442481573921522)\n",
      "iter27\n",
      "(0.11071266808975493, -0.05527692940536537)\n",
      "(1.3401483501087292, -0.18818808774568402)\n",
      "iter28\n",
      "(0.10793868261120802, -0.05389192646540066)\n",
      "(1.327969956618856, -0.18210762551109383)\n",
      "iter29\n",
      "(0.10523420147174971, -0.05254162548473842)\n",
      "(1.3160967015316232, -0.17617951359989975)\n",
      "iter30\n",
      "(0.10259748299657401, -0.051225157368825144)\n",
      "(1.3045209393697308, -0.17039993479657853)\n",
      "iter31\n",
      "(0.10002682941475687, -0.049941674268195714)\n",
      "(1.2932352162401077, -0.16476516748600775)\n",
      "iter32\n",
      "(0.09752058539674288, -0.048690349772142774)\n",
      "(1.2822322650044844, -0.15927158331650623)\n",
      "iter33\n",
      "(0.09507713712432624, -0.04747037810441137)\n",
      "(1.2715050006108426, -0.15391564484157053)\n",
      "iter34\n",
      "(0.09269491120146939, -0.04628097370449732)\n",
      "(1.2610465155271666, -0.14869390325008527)\n",
      "iter35\n",
      "(0.0903723736595635, -0.04512137068494191)\n",
      "(1.250850075295005, -0.14360299614259056)\n",
      "iter36\n",
      "(0.08810802896289413, -0.04399082235173244)\n",
      "(1.2409091141924529, -0.13863964536724696)\n",
      "iter37\n",
      "(0.0859004190481273, -0.04288860071848939)\n",
      "(1.2312172310065346, -0.1338006549085564)\n",
      "iter38\n",
      "(0.08374812238450896, -0.04181399603954257)\n",
      "(1.2217681849112405, -0.12908290882952256)\n",
      "iter39\n",
      "(0.08164975305884918, -0.040766316352227716)\n",
      "(1.2125558914489445, -0.12448336926517288)\n",
      "iter40\n",
      "(0.07960395988297413, -0.039744887031569136)\n",
      "(1.203574418612471, -0.11999907446642782)\n",
      "iter41\n",
      "(0.07760942552371153, -0.03874905035578056)\n",
      "(1.1948179830253438, -0.11562713689295522)\n",
      "iter42\n",
      "(0.07566486565461432, -0.037778165082773395)\n",
      "(1.1862809462177355, -0.11136474135381935)\n",
      "iter43\n",
      "(0.07376902812895594, -0.03683160603724097)\n",
      "(1.1779578109957278, -0.10720914319471428)\n",
      "iter44\n",
      "(0.07192069217344776, -0.035908763708087564)\n",
      "(1.1698432179015428, -0.10315766653061777)\n",
      "iter45\n",
      "(0.07011866760214719, -0.03500904385596463)\n",
      "(1.1619319417624636, -0.09920770252272813)\n",
      "iter46\n",
      "(0.06836179405008065, -0.034131867130606244)\n",
      "(1.1542188883262274, -0.09535670769857202)\n",
      "iter47\n",
      "(0.06664894022604757, -0.03327666869779171)\n",
      "(1.1466990909807186, -0.09160220231420534)\n",
      "iter48\n",
      "(0.06497900318416294, -0.03244289787562239)\n",
      "(1.1393677075558535, -0.08794176875744825)\n",
      "iter49\n",
      "(0.0633509076136413, -0.03163001777993256)\n",
      "(1.1322200172055956, -0.08437304999112978)\n",
      "iter50\n",
      "(0.06176360514638148, -0.030837504978573682)\n",
      "(1.125251417368095, -0.08089374803533719)\n",
      "iter51\n",
      "(0.06021607368189843, -0.030064849154363672)\n",
      "(1.118457420801993, -0.07750162248769409)\n",
      "iter52\n",
      "(0.058707316729166556, -0.029311552776487643)\n",
      "(1.1118336526969843, -0.07419448908071408)\n",
      "iter53\n",
      "(0.0572363627649589, -0.02857713078012104)\n",
      "(1.105375847856776, -0.07097021827530044)\n",
      "iter54\n",
      "(0.05580226460826304, -0.027861110254081962)\n",
      "(1.0990798479526305, -0.06782673388948712)\n",
      "iter55\n",
      "(0.05440409881036212, -0.02716303013632605)\n",
      "(1.0929415988457216, -0.0647620117615381)\n",
      "iter56\n",
      "(0.05304096506021292, -0.026482440917043548)\n",
      "(1.0869571479765818, -0.06177407844654224)\n",
      "iter57\n",
      "(0.05171198560471173, -0.025818904349216003)\n",
      "(1.0811226418199584, -0.058861009945667446)\n",
      "iter58\n",
      "(0.05041630468348675, -0.025171993166422456)\n",
      "(1.07543432340344, -0.056020930467253684)\n",
      "iter59\n",
      "(0.04915308797785991, -0.024541290807702572)\n",
      "(1.0698885298882566, -0.053252011218947216)\n",
      "iter60\n",
      "(0.04792152207360275, -0.02392639114933736)\n",
      "(1.064481690210692, -0.050552469230099935)\n",
      "iter61\n",
      "(0.046720813937167927, -0.023326898243321847)\n",
      "(1.0592103227825957, -0.047920566203672826)\n",
      "iter62\n",
      "(0.04555019040502937, -0.022742426062419503)\n",
      "(1.0540710332495073, -0.04535460739690742)\n",
      "iter63\n",
      "(0.04440889768583121, -0.02217259825157723)\n",
      "(1.0490605123049541, -0.042852940530041274)\n",
      "iter64\n",
      "(0.04329620087499948, -0.02161704788559149)\n",
      "(1.0441755335595126, -0.04041395472236778)\n",
      "iter65\n",
      "(0.042211383481521904, -0.021075417232832717)\n",
      "(1.0394129514632626, -0.038036079454952715)\n",
      "iter66\n",
      "(0.041153746966584207, -0.02054735752489186)\n",
      "(1.0347696992802953, -0.03571778355934112)\n",
      "iter67\n",
      "(0.040122610293761074, -0.020032528732007043)\n",
      "(1.030242787113971, -0.03345757423160301)\n",
      "iter68\n",
      "(0.03911730949048603, -0.01953059934410219)\n",
      "(1.0258292999816572, -0.031253996071082236)\n",
      "iter69\n",
      "(0.03813719722050184, -0.019041246157323954)\n",
      "(1.0215263959377037, -0.029105630143230997)\n",
      "iter70\n",
      "(0.03718164236702296, -0.01856415406592927)\n",
      "(1.0173313042434484, -0.027011093065925362)\n",
      "iter71\n",
      "(0.03625002962635252, -0.01809901585936703)\n",
      "(1.013241323583076, -0.024969036118673144)\n",
      "iter72\n",
      "(0.03534175911166524, -0.017645532024474924)\n",
      "(1.0092538203241772, -0.02297814437414277)\n",
      "iter73\n",
      "(0.034456245966733814, -0.01720341055260253)\n",
      "(1.0053662268218941, -0.02103713585145053)\n",
      "iter74\n",
      "(0.033592919989320597, -0.016772366751591113)\n",
      "(1.0015760397655533, -0.01914476069066425)\n",
      "iter75\n",
      "(0.032751225264019204, -0.016352123062439283)\n",
      "(0.9978808185667281, -0.01729980034798923)\n",
      "iter76\n",
      "(0.03193061980427729, -0.015942408880596493)\n",
      "(0.994278183787686, -0.015501066811120907)\n",
      "iter77\n",
      "(0.031130575203408506, -0.015542960381695703)\n",
      "(0.9907658156092155, -0.013747401834255293)\n",
      "iter78\n",
      "(0.030350576294332375, -0.01515352035168438)\n",
      "(0.9873414523368406, -0.012037676192268766)\n",
      "iter79\n",
      "(0.029590120817853328, -0.014773838021185693)\n",
      "(0.984002888944464, -0.010370788953583484)\n",
      "iter80\n",
      "(0.028848719099236208, -0.014403668904037348)\n",
      "(0.9807479756545001, -0.008745666771253057)\n",
      "iter81\n",
      "(0.028125893732901298, -0.0140427746398473)\n",
      "(0.9775746165535841, -0.007161263191808949)\n",
      "iter82\n",
      "(0.02742117927500608, -0.013690922840518215)\n",
      "(0.974480768242965, -0.005616557981425746)\n",
      "iter83\n",
      "(0.026734121943738955, -0.013347886940600916)\n",
      "(0.9714644385227144, -0.004110556468968742)\n",
      "iter84\n",
      "(0.026064279327116458, -0.013013446051407963)\n",
      "(0.9685236851089031, -0.002642288905502641)\n",
      "iter85\n",
      "(0.02541122009810632, -0.012687384818774072)\n",
      "(0.9656566143829203, -0.0012108098398477653)\n",
      "iter86\n",
      "(0.024774523736883113, -0.012369493284389955)\n",
      "(0.9628613801721286, 0.00018480249021738272)\n",
      "iter87\n",
      "(0.024153780260047848, -0.012059566750602135)\n",
      "(0.9601361825610715, 0.0015454467515002778)\n",
      "iter88\n",
      "(0.023548589956631567, -0.011757405648600294)\n",
      "(0.9574792667324662, 0.0028719990940665125)\n",
      "iter89\n",
      "(0.022958563130707887, -0.011462815409919174)\n",
      "(0.9548889218372367, 0.004165313715412545)\n",
      "iter90\n",
      "(0.02238331985046416, -0.011175606341141514)\n",
      "(0.9523634798928589, 0.005426223410503654)\n",
      "iter91\n",
      "(0.021822489703549962, -0.01089559350175821)\n",
      "(0.9499013147093078, 0.00665554010802922)\n",
      "iter92\n",
      "(0.021275711558563306, -0.010622596585075555)\n",
      "(0.9475008408419173, 0.007854055393222623)\n",
      "iter93\n",
      "(0.02074263333250937, -0.010356439802112566)\n",
      "(0.9451605125704754, 0.009022541017580935)\n",
      "iter94\n",
      "(0.02022291176408328, -0.010096951768410967)\n",
      "(0.9428788229038993, 0.010161749395813317)\n",
      "iter95\n",
      "(0.01971621219264148, -0.009843965393668237)\n",
      "(0.9406543026098502, 0.011272414090338523)\n",
      "iter96\n",
      "(0.019222208342700497, -0.009597317774153796)\n",
      "(0.9384855192686596, 0.012355250283642028)\n",
      "iter97\n",
      "(0.018740582113844786, -0.009356850087804425)\n",
      "(0.9363710763509626, 0.013410955238798946)\n",
      "iter98\n",
      "(0.018271023375892335, -0.009122407491958823)\n",
      "(0.9343096123184397, 0.014440208748457433)\n",
      "iter99\n",
      "(0.01781322976919692, -0.00889383902364722)\n",
      "(0.9322997997470915, 0.015443673572572904)\n",
      "iter100\n",
      "(0.017366906509947523, -0.008670997502390709)\n",
      "(0.9303403444724799, 0.016421995865174097)\n",
      "iter101\n",
      "(0.016931766200355823, -0.00845373943541769)\n",
      "(0.9284299847563857, 0.017375805590437075)\n",
      "iter102\n",
      "(0.016507528643590803, -0.008241924925272143)\n",
      "(0.9265674904743466, 0.01830571692833302)\n",
      "iter103\n",
      "(0.016093920663352218, -0.008035417579732796)\n",
      "(0.9247516623235517, 0.019212328670112955)\n",
      "iter104\n",
      "(0.015690675927971423, -0.007834084423977683)\n",
      "(0.9229813310505829, 0.020096224603883564)\n",
      "iter105\n",
      "(0.015297534778907652, -0.007637795814969872)\n",
      "(0.921255356698506, 0.02095797389052111)\n",
      "iter106\n",
      "(0.014914244063553975, -0.0074464253579673125)\n",
      "(0.9195726278728261, 0.021798131430167796)\n",
      "iter107\n",
      "(0.014540556972221582, -0.007259849825142823)\n",
      "(0.9179320610258351, 0.0226172382195442)\n",
      "iter108\n",
      "(0.014176232879217699, -0.007077949076226742)\n",
      "(0.9163325997588908, 0.02341582170030991)\n",
      "iter109\n",
      "(0.013821037187896726, -0.006900605981155028)\n",
      "(0.9147732141421768, 0.02419439609869485)\n",
      "iter110\n",
      "(0.013474741179602266, -0.006727706344637163)\n",
      "(0.9132529000515082, 0.024953462756621903)\n",
      "iter111\n",
      "(0.01313712186638703, -0.00655913883263014)\n",
      "(0.9117706785217519, 0.02569351045453199)\n",
      "iter112\n",
      "(0.01280796184742779, -0.006394794900643291)\n",
      "(0.9103255951164493, 0.026415015726121305)\n",
      "iter113\n",
      "(0.012487049169033898, -0.006234568723848255)\n",
      "(0.9089167193132323, 0.027118443165192067)\n",
      "iter114\n",
      "(0.012174177188166563, -0.006078357128933609)\n",
      "(0.9075431439046385, 0.027804245724815376)\n",
      "iter115\n",
      "(0.011869144439378535, -0.0059260595276659)\n",
      "(0.9062039844139402, 0.028472865008998073)\n",
      "iter116\n",
      "(0.011571754505081184, -0.00577757785212728)\n",
      "(0.9048983785256086, 0.029124731557041324)\n",
      "iter117\n",
      "(0.011281815889072034, -0.005632816491557313)\n",
      "(0.9036254855300497, 0.029760265120775325)\n",
      "iter118\n",
      "(0.010999141893221398, -0.005491682230795725)\n",
      "(0.9023844857822518, 0.03037987493484663)\n",
      "iter119\n",
      "(0.010723550497257317, -0.005354084190250907)\n",
      "(0.9011745801739974, 0.03098395998023416)\n",
      "iter120\n",
      "(0.010454864241554418, -0.005219933767388668)\n",
      "(0.8999949896192991, 0.03157290924116176)\n",
      "iter121\n",
      "(0.010192910112867128, -0.005089144579671701)\n",
      "(0.8988449545527282, 0.03214710195557451)\n",
      "iter122\n",
      "(0.009937519432920348, -0.004961632408940997)\n",
      "(0.8977237344403128, 0.0327069078593384)\n",
      "iter123\n",
      "(0.009688527749793582, -0.004837315147184412)\n",
      "(0.8966306073026916, 0.03325268742432191)\n",
      "iter124\n",
      "(0.009445774732028758, -0.004716112743662809)\n",
      "(0.8955648692502143, 0.03378479209051219)\n",
      "iter125\n",
      "(0.009209104065386017, -0.004597947153368287)\n",
      "(0.8945258340296911, 0.0343035644923151)\n",
      "iter126\n",
      "(0.00897836335219267, -0.004482742286763619)\n",
      "(0.8935128325824987, 0.03480933867918561)\n",
      "iter127\n",
      "(0.008753404013207141, -0.004370423960793607)\n",
      "(0.8925252126137575, 0.03530244033072961)\n",
      "iter128\n",
      "(0.008534081191949108, -0.004260919851109024)\n",
      "(0.8915623381723047, 0.035783186966416906)\n",
      "iter129\n",
      "(0.008320253661420144, -0.004154159445501687)\n",
      "(0.8906235892411902, 0.0362518881500389)\n",
      "iter130\n",
      "(0.008111783733167306, -0.004050073998494224)\n",
      "(0.889708361338434, 0.03670884568904409)\n",
      "iter131\n",
      "(0.007908537168617854, -0.003948596487082079)\n",
      "(0.8888160651277855, 0.03715435382887845)\n",
      "iter132\n",
      "(0.007710383092646403, -0.003849661567563084)\n",
      "(0.8879461260392376, 0.037588699442457484)\n",
      "iter133\n",
      "(0.0075171939092945035, -0.00375320553347504)\n",
      "(0.8870979838990465, 0.03801216221488942)\n",
      "iter134\n",
      "(0.007328845219613791, -0.0036591662745631194)\n",
      "(0.886271092569024, 0.038425014823571677)\n",
      "iter135\n",
      "(0.007145215741561417, -0.0035674832367890152)\n",
      "(0.8854649195948665, 0.03882752311377362)\n",
      "iter136\n",
      "(0.006966187231901383, -0.003478097383342449)\n",
      "(0.8846789458632948, 0.03921994626982041)\n",
      "iter137\n",
      "(0.006791644410068009, -0.00339095115662042)\n",
      "(0.8839126652677856, 0.039602536981988075)\n",
      "iter138\n",
      "(0.0066214748839328345, -0.003305988441167418)\n",
      "(0.8831655843826781, 0.03997554160921632)\n",
      "iter139\n",
      "(0.006455569077432388, -0.003223154527541766)\n",
      "(0.8824372221454455, 0.040339200337744734)\n",
      "iter140\n",
      "(0.006293820160011035, -0.0031423960770845193)\n",
      "(0.8817271095469279, 0.04069374733577433)\n",
      "iter141\n",
      "(0.006136123977828425, -0.0030636610875768855)\n",
      "(0.8810347893293267, 0.04103941090425362)\n",
      "iter142\n",
      "(0.0059823789866941205, -0.00298689885975163)\n",
      "(0.8803598156917656, 0.04137641362388708)\n",
      "iter143\n",
      "(0.005832486186679531, -0.002912059964649533)\n",
      "(0.8797017540032293, 0.04170497249845976)\n",
      "iter144\n",
      "(0.005686349058371551, -0.0028390962117874454)\n",
      "(0.8790601805226945, 0.04202529909457121)\n",
      "iter145\n",
      "(0.005543873500718929, -0.00276796061813126)\n",
      "(0.8784346821262736, 0.04233759967786783)\n",
      "iter146\n",
      "(0.005404967770440497, -0.0026986073778395847)\n",
      "(0.8778248560411945, 0.042642075345862264)\n",
      "iter147\n",
      "(0.005269542422949147, -0.0026309918327688587)\n",
      "(0.8772303095864461, 0.04293892215742462)\n",
      "iter148\n",
      "(0.0051375102547553285, -0.002565070443718398)\n",
      "(0.8766506599199217, 0.043228331259029196)\n",
      "iter149\n",
      "(0.005008786247316102, -0.0025008007623928464)\n",
      "(0.8760855337918987, 0.04351048900783822)\n",
      "iter150\n",
      "(0.0048832875122885675, -0.0024381414040700045)\n",
      "(0.8755345673046939, 0.04378557709170144)\n",
      "iter151\n",
      "(0.004760933238158113, -0.002377052020949433)\n",
      "(0.8749974056783422, 0.04405377264614914)\n",
      "iter152\n",
      "(0.004641644638198478, -0.0023174932761776124)\n",
      "(0.8744737030221448, 0.044315248368453575)\n",
      "iter153\n",
      "(0.00452534489974353, -0.002259426818510113)\n",
      "(0.8739631221119429, 0.04457017262883311)\n",
      "iter154\n",
      "(0.004411959134722301, -0.0022028152576231306)\n",
      "(0.8734653341729711, 0.04481870957886922)\n",
      "iter155\n",
      "(0.0043014143314383775, -0.0021476221400336614)\n",
      "(0.8729800186681517, 0.04506101925720776)\n",
      "iter156\n",
      "(0.004193639307556998, -0.0020938119256251053)\n",
      "(0.8725068630916935, 0.04529725769261147)\n",
      "iter157\n",
      "(0.004088564664265268, -0.002041349964768302)\n",
      "(0.8720455627678622, 0.045527577004430234)\n",
      "iter158\n",
      "(0.003986122741590767, -0.0019902024759994585)\n",
      "(0.871595820654793, 0.04575212550055475)\n",
      "iter159\n",
      "(0.003886247574827237, -0.001940336524279468)\n",
      "(0.871157347153218, 0.04597104777291469)\n",
      "iter160\n",
      "(0.003788874852063151, -0.0018917199997764533)\n",
      "(0.870729859919987, 0.046184484790585435)\n",
      "iter161\n",
      "(0.0036939418727680888, -0.0018443215971941929)\n",
      "(0.87031308368626, 0.04639257399056085)\n",
      "iter162\n",
      "(0.0036013875074175613, -0.0017981107956142773)\n",
      "(0.8699067500802555, 0.046595449366252206)\n",
      "iter163\n",
      "(0.003511152158131836, -0.0017530578388407104)\n",
      "(0.8695105974544396, 0.04679324155376978)\n",
      "iter164\n",
      "(0.003423177720298795, -0.0017091337162407016)\n",
      "(0.8691243707170451, 0.04698607791604226)\n",
      "iter165\n",
      "(0.003337407545158916, -0.0016663101440635541)\n",
      "(0.8687478211678122, 0.047174082624828736)\n",
      "iter166\n",
      "(0.0032537864033281546, -0.001624559547228499)\n",
      "(0.8683807063378448, 0.04735737674067573)\n",
      "iter167\n",
      "(0.0031722604492333715, -0.0015838550415695318)\n",
      "(0.8680227898334787, 0.047536078290870865)\n",
      "iter168\n",
      "(0.0030927771864432464, -0.0015441704165185532)\n",
      "(0.867673841184063, 0.04771030234544351)\n",
      "iter169\n",
      "(0.0030152854338603152, -0.0015054801182358302)\n",
      "(0.8673336356935543, 0.04788016109126055)\n",
      "iter170\n",
      "(0.002939735292765669, -0.001467759233152216)\n",
      "(0.8670019542958296, 0.04804576390426649)\n",
      "iter171\n",
      "(0.0028660781146909314, -0.0014309834719217263)\n",
      "(0.8666785834136254, 0.04820721741991324)\n",
      "iter172\n",
      "(0.0027942664700884245, -0.0013951291537884658)\n",
      "(0.8663633148210094, 0.048364625601824626)\n",
      "iter173\n",
      "(0.0027242541177918385, -0.0013601731913349346)\n",
      "(0.8660559455092997, 0.04851808980874136)\n",
      "iter174\n",
      "(0.0026559959752410376, -0.0013260930756138322)\n",
      "(0.8657562775563425, 0.0486677088597882)\n",
      "iter175\n",
      "(0.002589448089451554, -0.0012928668616568717)\n",
      "(0.8654641179990661, 0.04881357909810573)\n",
      "iter176\n",
      "(0.0025245676087126247, -0.001260473154342201)\n",
      "(0.8651792787092264, 0.04895579445288798)\n",
      "iter177\n",
      "(0.002461312754993339, -0.0012288910946189557)\n",
      "(0.864901576272268, 0.04909444649986562)\n",
      "iter178\n",
      "(0.002399642797042273, -0.0011981003460729723)\n",
      "(0.8646308318692187, 0.049229624520273706)\n",
      "iter179\n",
      "(0.0023395180241580354, -0.001168081081835176)\n",
      "(0.8643668711615441, 0.04936141555834173)\n",
      "iter180\n",
      "(0.002280899720619381, -0.0011388139718126855)\n",
      "(0.8641095241788868, 0.0494899044773436)\n",
      "iter181\n",
      "(0.0022237501407556064, -0.0011102801702413845)\n",
      "(0.8638586252096186, 0.049615174014243)\n",
      "iter182\n",
      "(0.002168032484640586, -0.0010824613035516962)\n",
      "(0.8636140126941355, 0.04973730483296955)\n",
      "iter183\n",
      "(0.002113710874396871, -0.0010553394585371579)\n",
      "(0.863375529120825, 0.04985637557636024)\n",
      "iter184\n",
      "(0.0020607503310931725, -0.0010288971708187987)\n",
      "(0.8631430209246413, 0.049972462916799326)\n",
      "iter185\n",
      "(0.0020091167522199417, -0.0010031174136014565)\n",
      "(0.8629163383882211, 0.05008564160558939)\n",
      "iter186\n",
      "(0.001958776889731395, -0.00097798358670661)\n",
      "(0.862695335545477, 0.05019598452108555)\n",
      "iter187\n",
      "(0.0019096983286342017, -0.0009534795058876222)\n",
      "(0.8624798700876065, 0.05030356271562328)\n",
      "iter188\n",
      "(0.0018618494661163942, -0.0009295893924045728)\n",
      "(0.8622698032714567, 0.05040844546127092)\n",
      "iter189\n",
      "(0.0018151994911981967, -0.0009062978628637142)\n",
      "(0.8620649998301839, 0.050510700294435426)\n",
      "iter190\n",
      "(0.0017697183648895498, -0.0008835899193158192)\n",
      "(0.8618653278861521, 0.05061039305935044)\n",
      "iter191\n",
      "(0.0017253768008493042, -0.0008614509395950265)\n",
      "(0.8616706588660142, 0.050707587950475176)\n",
      "iter192\n",
      "(0.0016821462465276022, -0.0008398666679028999)\n",
      "(0.8614808674179208, 0.05080234755383063)\n",
      "iter193\n",
      "(0.0016399988647773562, -0.0008188232056333788)\n",
      "(0.8612958313308028, 0.050894732887299954)\n",
      "iter194\n",
      "(0.0015989075159329401, -0.0007983070024168593)\n",
      "(0.8611154314556773, 0.050984803439919624)\n",
      "iter195\n",
      "(0.0015588457403317962, -0.0007783048473993026)\n",
      "(0.8609395516289247, 0.05107261721018548)\n",
      "iter196\n",
      "(0.0015197877412765654, -0.0007588038607352021)\n",
      "(0.8607680785974882, 0.051158230743399404)\n",
      "iter197\n",
      "(0.0014817083684261156, -0.0007397914852904086)\n",
      "(0.8606009019459477, 0.051241699168080274)\n",
      "iter198\n",
      "(0.0014445831015980875, -0.000721255478560534)\n",
      "(0.8604379140254209, 0.05132307623146222)\n",
      "iter199\n",
      "(0.0014083880349807065, -0.0007031839047862531)\n",
      "(0.8602790098842451, 0.05140241433410388)\n",
      "iter200\n",
      "(0.0013730998617409063, -0.0006855651272646424)\n",
      "(0.8601240872003972, 0.05147976456363036)\n"
     ]
    }
   ],
   "source": [
    "k, b = 3, 2\n",
    "n = 1\n",
    "while True:\n",
    "    step = loss_gradient(k, b, df)\n",
    "    step_k = step[0]\n",
    "    step_b = step[1]\n",
    "    print(\"iter\" + str(n))\n",
    "    print(step)\n",
    "    print((k, b))\n",
    "    new_k = k - 0.11*step_k\n",
    "    new_b = b - 0.11*step_b\n",
    "    if abs(step_k) + abs(step_b) < 0.001 or n == 200:\n",
    "        break\n",
    "    k = new_k\n",
    "    b = new_b\n",
    "    n = n + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8599730462156058, 0.05155517672762947)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_k, new_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the gradient on the loss at a single point (dL/dk, dL/db)\n",
    "def loss_gradient_at_single_point(k, b, x):\n",
    "    return ((x[0]*k+b - x[1])*2*x[0], (x[0]*k+b - x[1])*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_point = (0.7, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "k, b = 3, 2\n",
    "n = 1\n",
    "out_lst = [[] for i in range(50)]\n",
    "# How to choose the optimal learning rate and what is the exit condition of SGD\n",
    "\n",
    "while n <= 10000:\n",
    "    df1 = df.sample(frac=1)\n",
    "    for i in range(df1.shape[0]):\n",
    "        step = loss_gradient_at_single_point(k, b, df1.iloc[i])\n",
    "        step_k = step[0]\n",
    "        step_b = step[1]\n",
    "        \n",
    "        new_k = k - 0.1/np.log(n+1)*step_k\n",
    "        new_b = b - 0.1/np.log(n+1)*step_b\n",
    "        #new_k = k - 0.1*step_k\n",
    "        #new_b = b - 0.1*step_b\n",
    "        \n",
    "        # Keep track of the loss change at the test point by each training point\n",
    "        test_loss_change_byi = loss_at_test_point(k, b, test_point) - loss_at_test_point(new_k, new_b, test_point)\n",
    "        out_lst[df1.index[i]].append(test_loss_change_byi)\n",
    "        \n",
    "        k = new_k\n",
    "        b = new_b\n",
    "        \n",
    "        #print(\"iter\" + str(n))\n",
    "        #print((k, b))\n",
    "        \n",
    "        n = n + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8578472910127826, 0.052495529349279445)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_k,new_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TracInIdeal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_at_test_point(k, b, x):\n",
    "    return (x[0]*k+b - x[1])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_influence_df = pd.DataFrame(np.array(out_lst).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.258660</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.951251</td>\n",
       "      <td>-0.005929</td>\n",
       "      <td>0.096614</td>\n",
       "      <td>0.027173</td>\n",
       "      <td>0.033656</td>\n",
       "      <td>0.059228</td>\n",
       "      <td>0.143956</td>\n",
       "      <td>0.389391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098666</td>\n",
       "      <td>0.007230</td>\n",
       "      <td>0.066496</td>\n",
       "      <td>0.042631</td>\n",
       "      <td>0.100306</td>\n",
       "      <td>6.969121</td>\n",
       "      <td>-0.036883</td>\n",
       "      <td>0.018882</td>\n",
       "      <td>2.931238</td>\n",
       "      <td>0.108517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021626</td>\n",
       "      <td>-0.000395</td>\n",
       "      <td>-0.017618</td>\n",
       "      <td>-0.007909</td>\n",
       "      <td>-0.002698</td>\n",
       "      <td>0.024064</td>\n",
       "      <td>0.009063</td>\n",
       "      <td>0.023098</td>\n",
       "      <td>-0.019330</td>\n",
       "      <td>0.012135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003682</td>\n",
       "      <td>-0.005525</td>\n",
       "      <td>-0.023613</td>\n",
       "      <td>0.018657</td>\n",
       "      <td>0.019870</td>\n",
       "      <td>-0.008008</td>\n",
       "      <td>-0.034557</td>\n",
       "      <td>0.013819</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.003793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.021421</td>\n",
       "      <td>0.001932</td>\n",
       "      <td>-0.014734</td>\n",
       "      <td>-0.003313</td>\n",
       "      <td>-0.003080</td>\n",
       "      <td>0.022286</td>\n",
       "      <td>0.006813</td>\n",
       "      <td>0.024760</td>\n",
       "      <td>-0.014911</td>\n",
       "      <td>0.012993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005719</td>\n",
       "      <td>-0.003026</td>\n",
       "      <td>-0.017602</td>\n",
       "      <td>0.025126</td>\n",
       "      <td>0.016591</td>\n",
       "      <td>-0.005728</td>\n",
       "      <td>-0.028177</td>\n",
       "      <td>0.014512</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>0.005322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.015560</td>\n",
       "      <td>-0.000546</td>\n",
       "      <td>-0.013168</td>\n",
       "      <td>-0.005094</td>\n",
       "      <td>-0.002784</td>\n",
       "      <td>0.017773</td>\n",
       "      <td>0.004740</td>\n",
       "      <td>0.020598</td>\n",
       "      <td>-0.013257</td>\n",
       "      <td>0.010584</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>-0.003532</td>\n",
       "      <td>-0.015133</td>\n",
       "      <td>0.017198</td>\n",
       "      <td>0.016744</td>\n",
       "      <td>-0.007033</td>\n",
       "      <td>-0.025033</td>\n",
       "      <td>0.010922</td>\n",
       "      <td>-0.001118</td>\n",
       "      <td>0.003421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.015306</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>-0.013122</td>\n",
       "      <td>-0.004417</td>\n",
       "      <td>-0.002545</td>\n",
       "      <td>0.017372</td>\n",
       "      <td>0.005352</td>\n",
       "      <td>0.019808</td>\n",
       "      <td>-0.011882</td>\n",
       "      <td>0.008917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002867</td>\n",
       "      <td>-0.002889</td>\n",
       "      <td>-0.014375</td>\n",
       "      <td>0.015329</td>\n",
       "      <td>0.013791</td>\n",
       "      <td>-0.005715</td>\n",
       "      <td>-0.023493</td>\n",
       "      <td>0.011852</td>\n",
       "      <td>-0.000857</td>\n",
       "      <td>0.000919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.001917</td>\n",
       "      <td>-0.001403</td>\n",
       "      <td>-0.002338</td>\n",
       "      <td>-0.002340</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>-0.000099</td>\n",
       "      <td>0.008415</td>\n",
       "      <td>-0.002115</td>\n",
       "      <td>0.004551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>0.003355</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>-0.000625</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.004979</td>\n",
       "      <td>0.003411</td>\n",
       "      <td>-0.002826</td>\n",
       "      <td>-0.002898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.001833</td>\n",
       "      <td>-0.001446</td>\n",
       "      <td>-0.002383</td>\n",
       "      <td>-0.002165</td>\n",
       "      <td>0.000307</td>\n",
       "      <td>0.003110</td>\n",
       "      <td>-0.000226</td>\n",
       "      <td>0.007850</td>\n",
       "      <td>-0.002095</td>\n",
       "      <td>0.004272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>-0.000813</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-0.004929</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>-0.002982</td>\n",
       "      <td>-0.002892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.002118</td>\n",
       "      <td>-0.001551</td>\n",
       "      <td>-0.002685</td>\n",
       "      <td>-0.002414</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>0.002495</td>\n",
       "      <td>-0.000442</td>\n",
       "      <td>0.007096</td>\n",
       "      <td>-0.002208</td>\n",
       "      <td>0.004129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>0.003276</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>-0.000856</td>\n",
       "      <td>-0.000594</td>\n",
       "      <td>-0.004957</td>\n",
       "      <td>0.002964</td>\n",
       "      <td>-0.003170</td>\n",
       "      <td>-0.003232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.001781</td>\n",
       "      <td>-0.001758</td>\n",
       "      <td>-0.002431</td>\n",
       "      <td>-0.002522</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.002606</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.007286</td>\n",
       "      <td>-0.002519</td>\n",
       "      <td>0.004139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>0.002863</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>-0.000137</td>\n",
       "      <td>-0.000866</td>\n",
       "      <td>-0.000367</td>\n",
       "      <td>-0.004949</td>\n",
       "      <td>0.003128</td>\n",
       "      <td>-0.003101</td>\n",
       "      <td>-0.003124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.002131</td>\n",
       "      <td>-0.001570</td>\n",
       "      <td>-0.002451</td>\n",
       "      <td>-0.002315</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.002988</td>\n",
       "      <td>-0.000096</td>\n",
       "      <td>0.008284</td>\n",
       "      <td>-0.002354</td>\n",
       "      <td>0.004409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>0.003249</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>-0.000564</td>\n",
       "      <td>-0.000047</td>\n",
       "      <td>-0.004825</td>\n",
       "      <td>0.003196</td>\n",
       "      <td>-0.002991</td>\n",
       "      <td>-0.003216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0    0.258660  0.003903  0.951251 -0.005929  0.096614  0.027173  0.033656   \n",
       "1    0.021626 -0.000395 -0.017618 -0.007909 -0.002698  0.024064  0.009063   \n",
       "2    0.021421  0.001932 -0.014734 -0.003313 -0.003080  0.022286  0.006813   \n",
       "3    0.015560 -0.000546 -0.013168 -0.005094 -0.002784  0.017773  0.004740   \n",
       "4    0.015306  0.000866 -0.013122 -0.004417 -0.002545  0.017372  0.005352   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "195  0.001917 -0.001403 -0.002338 -0.002340  0.000430  0.002857 -0.000099   \n",
       "196  0.001833 -0.001446 -0.002383 -0.002165  0.000307  0.003110 -0.000226   \n",
       "197  0.002118 -0.001551 -0.002685 -0.002414 -0.000026  0.002495 -0.000442   \n",
       "198  0.001781 -0.001758 -0.002431 -0.002522  0.000299  0.002606 -0.000014   \n",
       "199  0.002131 -0.001570 -0.002451 -0.002315  0.000476  0.002988 -0.000096   \n",
       "\n",
       "           7         8         9   ...        40        41        42  \\\n",
       "0    0.059228  0.143956  0.389391  ...  0.098666  0.007230  0.066496   \n",
       "1    0.023098 -0.019330  0.012135  ...  0.003682 -0.005525 -0.023613   \n",
       "2    0.024760 -0.014911  0.012993  ...  0.005719 -0.003026 -0.017602   \n",
       "3    0.020598 -0.013257  0.010584  ...  0.001529 -0.003532 -0.015133   \n",
       "4    0.019808 -0.011882  0.008917  ...  0.002867 -0.002889 -0.014375   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "195  0.008415 -0.002115  0.004551  ...  0.001538  0.003355  0.000421   \n",
       "196  0.007850 -0.002095  0.004272  ...  0.001417  0.003048  0.000221   \n",
       "197  0.007096 -0.002208  0.004129  ...  0.001066  0.003276  0.000265   \n",
       "198  0.007286 -0.002519  0.004139  ...  0.001339  0.002863  0.000111   \n",
       "199  0.008284 -0.002354  0.004409  ...  0.001599  0.003249  0.000265   \n",
       "\n",
       "           43        44        45        46        47        48        49  \n",
       "0    0.042631  0.100306  6.969121 -0.036883  0.018882  2.931238  0.108517  \n",
       "1    0.018657  0.019870 -0.008008 -0.034557  0.013819  0.002382  0.003793  \n",
       "2    0.025126  0.016591 -0.005728 -0.028177  0.014512 -0.000077  0.005322  \n",
       "3    0.017198  0.016744 -0.007033 -0.025033  0.010922 -0.001118  0.003421  \n",
       "4    0.015329  0.013791 -0.005715 -0.023493  0.011852 -0.000857  0.000919  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "195  0.000114 -0.000625 -0.000041 -0.004979  0.003411 -0.002826 -0.002898  \n",
       "196 -0.000154 -0.000813 -0.000076 -0.004929  0.003255 -0.002982 -0.002892  \n",
       "197  0.000046 -0.000856 -0.000594 -0.004957  0.002964 -0.003170 -0.003232  \n",
       "198 -0.000137 -0.000866 -0.000367 -0.004949  0.003128 -0.003101 -0.003124  \n",
       "199  0.000022 -0.000564 -0.000047 -0.004825  0.003196 -0.002991 -0.003216  \n",
       "\n",
       "[200 rows x 50 columns]"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The loss reduced at test point by each training point in each epoch\n",
    "sgd_influence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.004801298320034"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ideal_loss_at_0 = loss_at_test_point(3, 2, test_point) - loss_at_test_point(new_k, new_b, test_point)\n",
    "ideal_loss_at_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.004801298320034"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_influence_df.sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, Lemma 3.1 in \"Estimating Training Data Influence by Tracing Gradient Descent\" paper is verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46   -1.535057\n",
       "27   -1.031383\n",
       "19   -1.004610\n",
       "28   -0.856976\n",
       "17   -0.855351\n",
       "dtype: float64"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_influence_df.sum(axis = 0).sort_values(ascending = True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.202459\n",
       "1    0.443936\n",
       "Name: 46, dtype: float64"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.204391\n",
       "1    0.354361\n",
       "Name: 27, dtype: float64"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[27]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify from the plot that #46, #27, #19 are indeed influential point for test_point = (0.7, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53276614, 0.97582032],\n",
       "       [0.97582032, 2.        ]])"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hessian = np.array([[sum(df[0]*df[0]*2), sum(2*df[0])], [sum(2*df[0]), 100]])/50\n",
    "Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17.65116314, -8.61218187],\n",
       "       [-8.61218187,  4.70197105]])"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_Hessian = np.linalg.inv(Hessian)\n",
    "inverse_Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the influence of each point on test point \n",
    "influence_score_lst = []\n",
    "loss_gradient_at_test = np.array(loss_gradient_at_single_point(new_k, new_b, test_point))\n",
    "test_gradient_inv_hes = np.matmul(loss_gradient_at_test.T, inverse_Hessian)\n",
    "\n",
    "for i in range(50):\n",
    "    \n",
    "    loss_gradient_at_train = np.array(loss_gradient_at_single_point(new_k, new_b, df.iloc[i]))\n",
    "    Infi_on_test = np.matmul(test_gradient_inv_hes, loss_gradient_at_train)\n",
    "    influence_score_lst.append(-Infi_on_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_df_all = pd.DataFrame(sgd_influence_df.sum(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_df_all[\"hessian\"] = influence_score_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_df_all.columns = [\"TracInIdeal\", \"Influence_function\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't understand why there is such a big difference between influence function and TracInIdeal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TracInIdeal</th>\n",
       "      <th>Influence_function</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.982819</td>\n",
       "      <td>-0.397896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-1.535057</td>\n",
       "      <td>-0.224364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.953930</td>\n",
       "      <td>-0.213753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.679626</td>\n",
       "      <td>-0.207289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.857514</td>\n",
       "      <td>-0.186942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    TracInIdeal  Influence_function\n",
       "7      1.982819           -0.397896\n",
       "46    -1.535057           -0.224364\n",
       "5      0.953930           -0.213753\n",
       "16     1.679626           -0.207289\n",
       "26     0.857514           -0.186942"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "influence_df_all.sort_values(by = \"Influence_function\", ascending = True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x15ef276a550>"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scatter(influence_df_all[\"TracInIdeal\"], influence_df_all[\"Influence_function\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check using influence function to estimate new parameters after removal of some point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_influence_onparam_lst = []\n",
    "\n",
    "for i in range(50):\n",
    "    \n",
    "    loss_gradient_at_train = np.array(loss_gradient_at_single_point(exact_theta_hat[0], exact_theta_hat[1], df.iloc[i]))\n",
    "    Infi_onparam = -np.matmul(inverse_Hessian, loss_gradient_at_train)\n",
    "    train_influence_onparam_lst.append(Infi_onparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_onparam_df = pd.DataFrame(np.array(train_influence_onparam_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.480450</td>\n",
       "      <td>0.169601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.206077</td>\n",
       "      <td>-0.040271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.479522</td>\n",
       "      <td>0.335040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.113045</td>\n",
       "      <td>0.032149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.034007</td>\n",
       "      <td>-0.031970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0 -0.480450  0.169601\n",
       "1  0.206077 -0.040271\n",
       "2 -0.479522  0.335040\n",
       "3  0.113045  0.032149\n",
       "4  0.034007 -0.031970"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "influence_onparam_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8540958829540322, 0.054489543186794664)"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_theta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.86370488, 0.05109753])"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimated_theta_hat_after_removing_0 = exact_theta_hat - 1/50*influence_onparam_df.iloc[0].values\n",
    "estimated_theta_hat_after_removing_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = LinearRegression()\n",
    "model2.fit(np.array([m.T[1:,0]]).T, m.T[1:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.86422264]), 0.05091475746923557)"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.coef_, model2.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That is really close to the estimated_theta_hat_after_removing_0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
